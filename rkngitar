#!/usr/bin/env python3

# Script to manage storage of uncompressed dump.xml + dump.xml.sig
# in git repo to utilize git delta-compression.
#
# Some metadata is also preserved as part of manifest files and git commit
# messages to simplify grep'ing:
# - size, mtime, MD5, SHA1, SHA256, SHA512 of dump.xml, dump.xml.sig
# - updateTime - both as UNIX time and as raw data
# - updateTimeUrgently - both as UNIX time and as raw data
# - signingTime (1.2.840.113549.1.9.5) from dump.xml.sig as UNIX time and in updateTime TZ
#
# Metadata of _original_ ZIP file is not preserved and is lost forever.
# Code _hopes_ that mtime of dump.xml{,.sig} is preserved by unzip.

import argparse
import binascii
import calendar
import codecs
import datetime
import functools
import hashlib
import json
import os
import random
import re
import shutil
import sys
import tempfile
import time
import traceback
import zipfile
from contextlib import contextmanager
from operator import itemgetter
from subprocess import run, check_call, check_output, Popen, PIPE # TODO replace check_call, check_output, with `run`
from tempfile import NamedTemporaryFile

import requests
import prometheus_client as prom # https://github.com/prometheus/client_python

GITAR_LATENCY = prom.Summary('gitar_duration_seconds', 'Step latency', ['step'])
GITAR_EXCEPTIONS = prom.Counter('gitar_exceptions', 'Step exceptions', ['step'])
HEAP_BYTES = prom.Gauge('gitar_heap_bytes', 'Non-packed .git/objects/?? size')
for step in ('todo', 'fetch', 'store', 'du', 'repack'):
    GITAR_LATENCY.labels(step)
    GITAR_EXCEPTIONS.labels(step)

OPT = None # CLI options

NAME = 'John Doe'
EMAIL = 'noreply@example.net'
RKN_EMAIL = 'noreply@rkn.gov.ru' # git needs email, but dump.xml.sig has no emails :-(
RKN_EPOCH = 1343462400

@contextmanager
def ScopedTmpdir(*args, **kwargs):
    tmpdir = tempfile.mkdtemp(*args, **kwargs)
    try:
        yield tmpdir
    finally:
        shutil.rmtree(tmpdir)

# Decorator is not used as @H.labels('foo').time() bar is SyntaxError, see following:
# - https://github.com/prometheus/client_python/issues/157
# - https://mail.python.org/pipermail/python-dev/2004-August/046711.html
@contextmanager
def counted_step(step):
    with GITAR_LATENCY.labels(step).time(), GITAR_EXCEPTIONS.labels(step).count_exceptions():
        yield

def git_init():
    if not os.path.exists(OPT.git_dir):
        check_call(['git', 'init', '--bare', OPT.git_dir])
        new = True
    else:
        new = False
    os.environ['GIT_DIR'] = os.path.abspath(OPT.git_dir)
    if new:
        check_call(['git', 'config', 'user.name', NAME])
        check_call(['git', 'config', 'user.email', EMAIL])
        empty_tree = check_output(['git', 'hash-object', '-t', 'tree', '/dev/null']).strip()
        commit = check_output(['git', 'commit-tree', '-m', 'Initial commit', empty_tree], env=dict(os.environ,
            GIT_AUTHOR_DATE='{:d} +0400'.format(RKN_EPOCH),
            GIT_COMMITTER_DATE='{:d} +0400'.format(RKN_EPOCH)
        )).strip()
        check_call(['git', 'update-ref', 'HEAD', commit])
    os.environ['GIT_COMMITTER_NAME'] = NAME
    os.environ['GIT_COMMITTER_EMAIL'] = EMAIL
    os.environ['GIT_AUTHOR_EMAIL'] = RKN_EMAIL
    # GIT_AUTHOR_NAME    ~ CN from dump.xml.sig
    # GIT_AUTHOR_DATE    ~ signingTime from dump.xml.sig
    # GIT_COMMITTER_DATE ~ signingTime

SIGNING_RE = re.compile(br'object: signingTime \(1\.2\.840\.113549\.1\.9\.5\)\s+(?:value\.)?set:\s+UTCTIME:(?P<mon>Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\s+(?P<day>\d+) (?P<hour>\d\d):(?P<min>\d\d):(?P<sec>\d\d) (?P<year>\d{4}) GMT\s', re.DOTALL)
MONMAP = {b'Jan': 1, b'Feb': 2, b'Mar': 3, b'Apr': 4, b'May': 5, b'Jun': 6,
          b'Jul': 7, b'Aug': 8, b'Sep': 9, b'Oct': 10, b'Nov': 11, b'Dec': 12}

def cms_signing_time(cms):
    m = SIGNING_RE.search(cms)
    if m is None:
        raise RuntimeError('Signature file without signingTime')
    d = m.groupdict()
    for k in d.keys():
        if k == 'mon':
            d[k] = MONMAP[d[k]] # NB, it's 1-12, not 0-11
        else:
            d[k] = int(d[k], 10)
    return calendar.timegm((d['year'], d['mon'], d['day'], d['hour'], d['min'], d['sec']))

SUBJECT_DN_RE = re.compile(br'^\s+subject: (?P<s_dn>.*)$', re.MULTILINE)
CN_RE = re.compile(br'\bCN=(?P<cn>.+?)(?:$|, |/street=)') # ugly, but works for now
# SHA1 c574234078c05931b952c930c2163e32b2af8a66 dump.xml.sig mixes `, ` and `/`
# as field separators in subject. That's a funny side-effect of `gost` module.

def cms_subject_cn(cms):
    # Sorry, RFC2253 and RFC4514 are mostly ignored here.
    m = SUBJECT_DN_RE.search(cms)
    if m is None:
        raise RuntimeError('Signature file withoug subject DN')
    dn = m.group('s_dn')
    m = CN_RE.search(dn)
    if m is None:
        raise RuntimeError('Signature file with subject DN without CN', dn)
    cn = m.group('cn') # bytes
    for enc in ('utf-8', 'utf-16-be'): # pure horor suitable only for `git log` :-(
        try:
            val = codecs.escape_decode(cn)[0].decode(enc) # str
        except Exception:
            continue
        if val.isprintable():
            return 'CN=' + val
    raise RuntimeError('CN with strange encoding', cn)

def file_githash(fname):
    h = hashlib.sha1()
    h.update('blob {:d}\0'.format(os.path.getsize(fname)).encode('ascii'))
    with open(fname, 'rb') as fd:
        for blob in iter(functools.partial(fd.read, 65536), b''):
            h.update(blob)
    return h.hexdigest()

def file_metadata(fname):
    blob = hashlib.sha1()
    blob.update('blob {:d}\0'.format(os.path.getsize(fname)).encode('ascii'))
    hashes = [hashlib.md5(), hashlib.sha1(), hashlib.sha256(), hashlib.sha512(), blob]
    with open(fname, 'rb') as fd:
        for blob in iter(functools.partial(fd.read, 65536), b''):
            for h in hashes:
                h.update(blob)
    return {
        'name': os.path.basename(fname),
        'size': os.path.getsize(fname),
        'mtime': int(os.path.getmtime(fname)),
        'MD5': hashes[0].hexdigest(),
        'SHA1': hashes[1].hexdigest(),
        'SHA256': hashes[2].hexdigest(),
        'SHA512': hashes[3].hexdigest(),
        'GIT': hashes[4].hexdigest(),
    }

def git_blob_exists(githash):
    p = run(['git', 'cat-file', '-e', githash])
    if p.returncode == 0:
        return True
    elif p.returncode == 1:
        return False
    else:
        raise RuntimeError('`git cat-file -e` failure', githash, p)

# Non-zero minutes TZ are not tested, so `:00` match is hard-coded
UPDATE_TIME_RE = re.compile(br'\bupdateTime="(?P<raw>(?P<year>\d{4})-(?P<mon>\d\d)-(?P<day>\d\d)T(?P<hour>\d\d):(?P<min>\d\d):(?P<sec>\d\d)\+(?P<tzh>\d\d):00)"')
UPDATE_TIME_URGENTLY_RE = re.compile(br'\bupdateTimeUrgently="(?P<raw>(?P<year>\d{4})-(?P<mon>\d\d)-(?P<day>\d\d)T(?P<hour>\d\d):(?P<min>\d\d):(?P<sec>\d\d)\+(?P<tzh>\d\d):00)"')

def update_timegm(re_match):
    # NB regexps have + offset hardcoded
    s = re_match.groupdict()
    d = {k: int(s[k], 10) for k in s if k != 'raw'}
    tz = datetime.timezone(datetime.timedelta(hours=d['tzh']))
    dt = datetime.datetime(d['year'], d['mon'], d['day'], d['hour'], d['min'], d['sec'], tzinfo=tz)
    return dt, re_match.group('raw').decode('ascii')

def update_time(dump_xml):
    with open(dump_xml, 'rb') as fd:
        head = fd.read(4096)
    m = UPDATE_TIME_RE.search(head)
    if m is None:
        raise RuntimeError('Dump file without updateTime', dump_xml, head)
    ut, utraw = update_timegm(m)
    m = UPDATE_TIME_URGENTLY_RE.search(head)
    if m is None:
        # That's FIXME as http://vigruzki.rkn.gov.ru/docs/description_for_operators_actual.pdf
        # allows that field to be absent.
        raise RuntimeError('FIXME: dump file without updateTimeUrgently', dump_xml, head)
    utu, uturaw = update_timegm(m)
    if ut.tzinfo != utu.tzinfo:
        raise RuntimeError('Insanity, updateTime and updateTimeUrgently in different timezones', utraw, uturaw)
    return ut, utraw, utu, uturaw

def isoformat(unix_ts, tz):
    return datetime.datetime.fromtimestamp(unix_ts, tz).isoformat()

def commit_files(dump_xml, dump_xml_sig, sanity_cb=None):
    cms = check_output(['openssl', 'cms', '-inform', 'DER', '-in', dump_xml_sig, '-cmsout', '-print'])
    signing_ts = cms_signing_time(cms)
    subject_cn = cms_subject_cn(cms)
    verify = Popen(['openssl', 'smime', '-verify', '-engine', 'gost', '-CApath', OPT.capath, '-attime', str(signing_ts),
                    '-in', dump_xml_sig, '-inform', 'DER', '-content', dump_xml, '-out', '/dev/null'], stderr=PIPE)
    # do hashing and sig-check in parallel
    sigmeta = file_metadata(dump_xml_sig)
    xmlmeta = file_metadata(dump_xml)
    ut, utraw, utu, uturaw = update_time(dump_xml)
    if sanity_cb is not None:
        sanity_cb(xmlmeta, sigmeta, ut, utu)
    # back to openssl
    stderr = verify.stderr.read()
    if verify.wait() != 0 or b'Verification successful\n' not in stderr:
        # `stderr` double check is needed because...
        ### $ openssl smime -verify -engine gost -CApath /nonexistent -in dump.xml.sig -inform DER && echo OKAY
        ### engine "gost" set.
        ### smime: Not a directory: /nonexistent
        ### smime: Use -help for summary.
        ### OKAY <--- ^!(*&^%@(^%@#&$%!!!
        # I hope, it has no messages like "Not Quite Verification successful\n"...
        raise RuntimeError('openssl smime -verify failure', signing_ts, stderr) # libengine-gost-openssl1.1 missing?
    # git does not preserve mtime, so mtimes are saved as json manifest all the
    # other fields are preserved, so they're only stored in commit messages.
    mtime_json = [{k: _[k] for k in ('name', 'mtime')} for _ in (xmlmeta, sigmeta)]
    mtime_json = json.dumps(mtime_json, sort_keys=True, separators=(',', ':')).encode('utf-8')
    mtime_blob = run(['git', 'hash-object', '-w', '--stdin'], input=mtime_json, stdout=PIPE, check=True).stdout.strip().decode('ascii')
    xmlblob, sigblob = run(['git', 'hash-object', '-w', '--', dump_xml, dump_xml_sig], stdout=PIPE, check=True).stdout.strip().decode('ascii').split()
    run(['git', 'read-tree', '--empty'], check=True)
    signing_utc = time.gmtime(signing_ts)
    run(['git', 'update-index', '--add',
         '--cacheinfo', '100644,{:s},meta.json'.format(mtime_blob),
         '--cacheinfo', '100644,{:s},dump.xml'.format(xmlblob),
         '--cacheinfo', '100644,{:s},dump.xml.sig'.format(sigblob),
    ], check=True)
    tree = run(['git', 'write-tree'], stdout=PIPE, check=True).stdout.strip().decode('ascii')
    # index is prepared with the proper tree, let's prepare commit
    signing_raw = isoformat(signing_ts, ut.tzinfo)
    body = [
        'Updated {:s}, signed {:s}'.format(utraw, signing_raw),
        '',
        '{:s} {:.0f} updateTime'.format(utraw, ut.timestamp()),
        '{:s} {:.0f} updateTimeUrgently'.format(uturaw, utu.timestamp()),
        '{:s} {:d} signingTime'.format(signing_raw, signing_ts),
        '{:s} {:d} dump.xml mtime'.format(isoformat(xmlmeta['mtime'], ut.tzinfo), xmlmeta['mtime']),
        '{:s} {:d} dump.xml.sig mtime'.format(isoformat(sigmeta['mtime'], ut.tzinfo), sigmeta['mtime']),
    ]
    for h in ('MD5', 'SHA1', 'GIT', 'SHA256', 'SHA512'):
        body.extend((
            '{:s} {:s} dump.xml'.format(h, xmlmeta[h]),
            '{:s} {:s} dump.xml.sig'.format(h, sigmeta[h]),
        ))
    body = '\n'.join(body).encode('utf-8')
    # do the commit, `git-commit` needs working tree, so it's not used
    head = run(['git', 'rev-parse', 'HEAD'], stdout=PIPE, check=True).stdout.strip().decode('ascii')
    git_ts = '{:d} +{:02.0f}00'.format(signing_ts, ut.tzinfo.utcoffset(None).seconds / 3600)
    commit = run(['git', 'commit-tree', '-p', head, tree], input=body, env=dict(os.environ,
            GIT_AUTHOR_NAME=subject_cn.encode('utf-8'),
            GIT_COMMITTER_DATE=git_ts,
            GIT_AUTHOR_DATE=git_ts), stdout=PIPE, check=True).stdout.strip().decode('ascii')
    run(['git', 'update-ref', 'HEAD', commit], check=True)

GIT_OBJDIR_RE = re.compile(r'[0-9a-fA-F]{2}')

def objects_heap_size():
    sz = 0
    # enumerating unpacked (or loose) objects
    objdir = os.path.join(OPT.git_dir, 'objects')
    for hh in filter(GIT_OBJDIR_RE.fullmatch, os.listdir(objdir)):
        hhdir = os.path.join(objdir, hh)
        for ff in os.listdir(hhdir):
            sz += os.path.getsize(os.path.join(hhdir, ff))
    return sz

def objects_repack():
    # incremental pack, `git gc` will do full-pack probably...
    run(['git', 'repack', '--window-memory={}'.format(OPT.window_memory)], check=True)
    run(['git', 'prune-packed'], check=True)

def load_known_dump_xml():
    local_sha256 = set()
    max_update_time = 0
    proc = Popen(['git', 'log', '--format=%b'], stdout=PIPE)
    for line in proc.stdout:
        if line.startswith(b'SHA256 ') and line.endswith(b' dump.xml\n'):
            local_sha256.add(binascii.unhexlify(line.split()[1]))
        elif line.endswith(b' updateTime\n'):
            max_update_time = max(max_update_time, int(line.split()[-2]))
    return max_update_time, local_sha256

def get_some_eais_dumps(sess, ts_start, local_sha256):
    todo = []
    # 4096 entries to get ~ 1 MiB of metadata per round-trip
    r = sess.get('https://{}/start?ts={:d}&c=4096'.format(OPT.eais_fqdn, ts_start))
    r.raise_for_status()
    for el in r.json():
        # `ts=` arg should be an index on `ut` field of response
        if binascii.unhexlify(el['id']) not in local_sha256:
            todo.append({k: el[k] for k in ('id', 'as', 'm', 'ut', 'utu')}) # see README.md for keys
    assert len({_['id'] for _ in todo}) == len(todo), 'Duplicate `id` in TODO list'
    return todo

def get_dump_zip(sess, zipfd, fileid):
    r = sess.get('https://{}/get/{}'.format(OPT.eais_fqdn, fileid), stream=True)
    r.raise_for_status()
    shutil.copyfileobj(r.raw, zipfd)
    zipfd.flush()

def zip_extract(zfd, tmpdir, fname, mtime):
    zfd.extract(fname, path=tmpdir)
    fpath = os.path.join(tmpdir, fname)
    os.utime(fpath, (mtime, mtime))
    return fpath

def commit_dump_zip(zipfd, xml_sha256, xml_size, xml_mtime, update_time, update_time_urgently):
    with zipfile.ZipFile(zipfd, 'r') as zfd, ScopedTmpdir() as tmpdir:
        xml = zip_extract(zfd, tmpdir, 'dump.xml', xml_mtime)
        sig = zip_extract(zfd, tmpdir, 'dump.xml.sig', RKN_EPOCH)
        assert not git_blob_exists(file_githash(sig)), 'dump.xml.sig is archived, but dump.xml is not'
        def sanity_cb(xmlmeta, sigmeta, ut, utu):
            assert xmlmeta['SHA256'] == xml_sha256
            assert xmlmeta['size'] == xml_size
            assert ut.timestamp() == update_time
            assert utu.timestamp() == update_time_urgently
        commit_files(xml, sig, sanity_cb)

def eais_loop():
    with open(OPT.eais_token) as fd:
        token = fd.read().strip()
    ts_start, local_sha256 = load_known_dump_xml()
    with requests.Session() as sess:
        sess.headers.update({
            'Authorization': 'Bearer {:s}'.format(token),
            'User-Agent': 'rkngitar/0.0; https://darkk.net.ru/',
        })
        while True:
            try:
                with counted_step('todo'):
                    todo = get_some_eais_dumps(sess, ts_start, local_sha256)
                todo.sort(key=itemgetter('ut')) # it _should_ be already sorted, but may be not
                for el in todo:
                    assert ts_start < el['ut'], (ts_start, el['ut'])
                    with NamedTemporaryFile(prefix='tmpzip') as zipfd:
                        with counted_step('fetch'):
                            get_dump_zip(sess, zipfd, el['id'])
                        with counted_step('store'):
                            commit_dump_zip(zipfd, el['id'], el['as'], el['m'], el['ut'], el['utu'])
                    ts_start = max(el['ut'], ts_start)
                    local_sha256.add(binascii.unhexlify(el['id']))
                    with counted_step('du'):
                        sz = objects_heap_size()
                    HEAP_BYTES.set(sz)
                    if OPT.objects_xmx < sz:
                        with counted_step('repack'):
                            objects_repack()
            except Exception:
                traceback.print_exc(file=sys.stderr)
            time.sleep(60)

def git_size(s):
    suffix = {
        'k': 1024,
        'm': 1024 * 1024,
        'g': 1024 * 1024 * 1024,
    }
    return int(s[:-1]) * suffix[s[-1]] if s.endswith(tuple(suffix.keys())) else int(s)

def parse_args():
    p = argparse.ArgumentParser(description='Importer from EAIS to local git archive')
    p.add_argument('--git-dir', help='Git repo directory', metavar='DIR', required=True)
    p.add_argument('--objects-xmx', help='Git/objects max size to start repack', metavar='SIZE', type=git_size, default='5g')
    p.add_argument('--window-memory', help='Window size for git delta compression', metavar='SIZE', type=git_size, default='384m')
    p.add_argument('--eais-fqdn', help='EAIS service FQDN', metavar='FQDN', required=True)
    p.add_argument('--eais-token', help='EAIS token file', metavar='FILE', required=True)
    p.add_argument('--capath', help='CA path with GOST certs', metavar='DIR', required=True)
    p.add_argument('--prom-addr', help='Prometheus exporter bind IP', metavar='IP', default='127.0.0.1')
    p.add_argument('--prom-port', help='Prometheus exporter bind port', metavar='PORT', type=int, required=True)
    return p.parse_args()

def main():
    global OPT
    OPT = parse_args()
    with counted_step('du'):
        HEAP_BYTES.set(objects_heap_size())
    prom.start_http_server(OPT.prom_port, OPT.prom_addr)
    git_init()
    eais_loop()

if __name__ == '__main__':
    main()
